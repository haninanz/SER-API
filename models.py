# -*- coding: utf-8 -*-
"""ravdes.ipynb
Automatically generated by Colaboratory.
Original file is located at
    https://colab.research.google.com/drive/1a9Z62r78oFB3iwUMXdBmadYHDlilR5fY
"""

## RAVDES SER: Speech emotion recongition using ravdess dataset
# source : https://mc.ai/speech-emotion-recognition-using-deep-neural-network-part-i/

# import needed packages

import numpy as np  
from keras.models import Sequential, Model
from keras.layers import Input, Dense, Activation, BatchNormalization, GRU, LSTM, Flatten, Conv1D, MaxPooling1D, Bidirectional
from keras.layers import Dropout  
from self_attention import SelfAttention
from sklearn.metrics import confusion_matrix  
import sklearn
import pandas as pd  
import seaborn as sns  
import matplotlib.pyplot as pl
import pandas as pd
import keras.backend as K

# def f1_score(y_true, y_pred): #taken from old keras source code
#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
#     precision = true_positives / (predicted_positives + K.epsilon())
#     recall = true_positives / (possible_positives + K.epsilon())
#     f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())
    
#     return f1_val

def create_model_dnn(n_dim, n_classes, dropout, loss, optimizer, activation_function='relu', init_type='normal'): 
    """
    Create deep neural network keras sequential model. This model using 4 layers inside
 
    Input shape: [batch_size, n_dim]
    Parameters:
      n_dim (int): the length of 2nd dimension of the data
      n_classes (int): the number of classes that the dataset have
      loss (string): loss function for evaluating the model while training
      optimizer (string or Keras Optimizer): the optimizer that will be used for optimizing weights based on previous loss
      activation_function (str): activation_function that will be used inside the model
        (default is relu)
      init_type (str): defininig how model initialize the model's kernel/weights
        (default is normal)
      optimiser (str): optimizer method that want to be used
        (default is adam)
      dropout_rate (float): rate of dropout
        (default is 0.5)
    Returns:
      model: a compiled Keras sequential DNN model
    """ 
    model = Sequential()  
    # layer 1  
    model.add(Dense(n_dim, input_dim=n_dim, kernel_initializer=init_type, activation=activation_function))  
    # layer 2  
    model.add(Dense(400, kernel_initializer=init_type, activation=activation_function))  
    model.add(Dropout(dropout))  
    # layer 3  
    model.add(Dense(200, kernel_initializer=init_type, activation=activation_function))  
    model.add(Dropout(dropout))  
    #layer4  
    model.add(Dense(100, kernel_initializer=init_type, activation=activation_function))  
    model.add(Dropout(dropout))  
    # output layer  
    model.add(Dense(n_classes, kernel_initializer=init_type, activation='softmax'))  
    # model compilation  
    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])  
    return model

def create_model_gru(n_timesteps, n_dim, n_classes, dropout, loss, optimizer):
    """
    Create Gated Recurrent Unit keras sequential model. This model using 3 layers GRU and
    2 dense layers
    Input shape: [batch_size, n_timesteps, n_dim]
    Parameters:
      n_timesteps: the length of 2nd dimension of the data
      n_dim (int): the length of 3rd dimension of the data
      n_classes (int): the number of classes that the dataset have
      loss (string): loss function for evaluating the model while training
      optimizer (string or Keras Optimizer): the optimizer that will be used for optimizing weights based on previous loss
    Returns:
      model: a compiled Keras sequential GRU model
    """
    model = Sequential()
    model.add(BatchNormalization(axis=-1, input_shape=(n_timesteps, n_dim)))
    model.add(GRU(n_dim, return_sequences=True, dropout=dropout, #input_shape=(1, 193),
                recurrent_dropout=0.2))  
    model.add(GRU(n_dim*2, dropout=dropout, recurrent_dropout=0.2, return_sequences=True))
    model.add(GRU(n_dim, dropout=dropout, recurrent_dropout=0.2))  
    # model.add(Flatten())
    model.add(Dense(n_dim, activation='sigmoid'))  
    #model.add(Dropout=0.4)
    model.add(Dense(n_classes, activation='softmax'))
            
    # model compilation  
    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])  
    return model

def create_model_lstm(n_timesteps, n_dim, n_classes, dropout, loss, optimizer):
    """
    Create Long Short Term Memory keras sequential model. This model using 3 layers LSTM and
    2 dense layers
    Input shape: [batch_size, n_timesteps, n_dim]
    Parameters:
      n_timesteps: the length of 2nd dimension of the data
      n_dim (int): the length of 3rd dimension of the data
      n_classes (int): the number of classes that the dataset have
      loss (string): loss function for evaluating the model while training
      optimizer (string or Keras Optimizer): the optimizer that will be used for optimizing weights based on previous loss
    Returns:
      model: a compiled Keras sequential LSTM model
    """

    model = Sequential()
    model.add(BatchNormalization(axis=-1, input_shape=(n_timesteps, n_dim)))
    model.add(LSTM(n_dim, return_sequences=True, dropout=0.5, #input_shape=(1, 193),
                 recurrent_dropout=0.2))  
    model.add(LSTM(n_dim*2, dropout=dropout, recurrent_dropout=0.2, return_sequences=True))
    model.add(LSTM(n_dim, dropout=dropout, recurrent_dropout=0.2, return_sequences=True))
    model.add(Flatten())
    model.add(Dense(n_dim, activation='sigmoid'))  
    #model.add(Dropout=0.4)
    model.add(Dense(n_classes, activation='softmax'))
              
    # model compilation  
    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])  
    return model

def create_model_bilstm(n_timesteps, n_dim, n_classes, dropout, loss, optimizer):
    """
    Create Bidirectional LSTM keras sequential model. This model using 3 layers BiLSTM and
    2 dense layers
    Input shape: [batch_size, n_timesteps, n_dim]
    Parameters:
      n_timesteps: the length of 2nd dimension of the data
      n_dim (int): the length of 3rd dimension of the data
      n_classes (int): the number of classes that the dataset have
      loss (string): loss function for evaluating the model while training
      optimizer (string or Keras Optimizer): the optimizer that will be used for optimizing weights based on previous loss
    Returns:
      model: a compiled Keras sequential BiLSTM model
    """

    model = Sequential()
    model.add(BatchNormalization(axis=-1, input_shape=(n_timesteps, n_dim)))
    model.add(Bidirectional(LSTM(n_dim, return_sequences=True, dropout=0.5, #input_shape=(1, 193),
                 recurrent_dropout=0.2)))  
    model.add(Bidirectional(LSTM(n_dim*2, dropout=dropout, recurrent_dropout=0.2, return_sequences=True)))
    model.add(Bidirectional(LSTM(n_dim, dropout=dropout, recurrent_dropout=0.2, return_sequences=True)))
    model.add(Flatten())
    model.add(Dense(n_dim, activation='sigmoid'))  
    #model.add(Dropout=0.4)
    model.add(Dense(n_classes, activation='softmax'))
              
    # model compilation  
    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy',f1_score])  
    return model

def create_model_cnn(n_timesteps, n_dim, n_classes, dropout, loss, optimizer):
    """
    Create Convolutional Neural Network keras sequential model. This model using 2 layers Conv1D and
    2 dense layers
    Input shape: [batch_size, n_timesteps, n_dim]
    Parameters:
      n_timesteps: the length of 2nd dimension of the data
      n_dim (int): the length of 3rd dimension of the data
      n_classes (int): the number of classes that the dataset have
      loss (string): loss function for evaluating the model while training
      optimizer (string or Keras Optimizer): the optimizer that will be used for optimizing weights based on previous loss
    Returns:
      model: a compiled Keras sequential GRU model
    """

    model = Sequential()
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu',data_format="channels_first", input_shape=(n_timesteps,n_dim)))
    model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
    model.add(Dropout(dropout))
    model.add(MaxPooling1D(pool_size=2))
    model.add(Flatten())
    model.add(Dense(100, activation='relu'))
    model.add(Dense(n_classes, activation='softmax'))
    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])

    return model

def create_model_cnn_attention(n_timesteps, n_dim, n_classes, dropout, loss, optimizer):
    """
    Create Convolutional Neural Network keras sequential model. This model using 2 layers Conv1D followed by
    1 Self Attention layer and ends with 2 dense layers
    Input shape: [batch_size, n_timesteps, n_dim]
    Parameters:
      n_timesteps: the length of 2nd dimension of the data
      n_dim (int): the length of 3rd dimension of the data
      n_classes (int): the number of classes that the dataset have
      loss (string): loss function for evaluating the model while training
      optimizer (string or Keras Optimizer): the optimizer that will be used for optimizing weights based on previous loss
    Returns:
      model: a compiled Keras sequential GRU model
    """

    model_input = Input(shape=(n_timesteps, n_dim))
    conv1 = Conv1D(filters=64, kernel_size=3, activation='relu',data_format="channels_first", input_shape=(n_timesteps, n_dim))(model_input)
    conv2 = Conv1D(filters=64, kernel_size=3, activation='relu')(conv1)
    dropout = Dropout(dropout)(conv2)
    maxpool = MaxPooling1D(pool_size=2)(dropout)
    attention = SelfAttention(32)(maxpool)
    dense1 = Densei(100, activation='relu')(attention)
    model_output = Dense(n_classes, activation='softmax')(dense1)

    model = Model(model_input, model_output)
    model.compile(loss=loss, optimizer=optimizer, metrics=['accuracy'])

    # model.summary()

    return model